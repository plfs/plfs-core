--- /tmp/openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs.c	1969-12-31 17:00:00.000000000 -0700
+++ ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs.c	2011-01-31 16:39:35.000000000 -0700
@@ -0,0 +1,91 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/* 
+ *   $Id: ad_plfs.c,v 1.1 2010/11/29 19:59:01 adamm Exp $
+ *
+ *   Copyright (C) 2001 University of Chicago. 
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "ad_plfs.h"
+
+/* adioi.h has the ADIOI_Fns_struct define */
+#include "adioi.h"
+
+struct ADIOI_Fns_struct ADIO_PLFS_operations = {
+    ADIOI_PLFS_Open, /* Open */
+    ADIOI_PLFS_ReadContig, /* ReadContig */
+    ADIOI_PLFS_WriteContig, /* WriteContig */
+    ADIOI_GEN_ReadStridedColl, /* ReadStridedColl */
+    ADIOI_GEN_WriteStridedColl, /* WriteStridedColl */
+    ADIOI_GEN_SeekIndividual, /* SeekIndividual */
+    ADIOI_PLFS_Fcntl, /* Fcntl */
+    ADIOI_PLFS_SetInfo, /* SetInfo */
+    ADIOI_GEN_ReadStrided, /* ReadStrided */
+    ADIOI_GEN_WriteStrided, /* WriteStrided */
+    ADIOI_PLFS_Close, /* Close */
+    ADIOI_FAKE_IreadContig, /* IreadContig */
+    ADIOI_FAKE_IwriteContig, /* IwriteContig */
+    ADIOI_FAKE_IODone, /* ReadDone */
+    ADIOI_FAKE_IODone, /* WriteDone */
+    ADIOI_FAKE_IOComplete, /* ReadComplete */
+    ADIOI_FAKE_IOComplete, /* WriteComplete */
+    ADIOI_FAKE_IreadStrided, /* IreadStrided */
+    ADIOI_FAKE_IwriteStrided, /* IwriteStrided */
+    ADIOI_PLFS_Flush, /* Flush */
+    ADIOI_PLFS_Resize, /* Resize */
+    ADIOI_PLFS_Delete, /* Delete */
+};
+
+int ad_plfs_amode( int access_mode ) {
+    int amode = 0; // O_META;
+    if (access_mode & ADIO_RDONLY)
+        amode = amode | O_RDONLY;
+    if (access_mode & ADIO_WRONLY)
+        amode = amode | O_WRONLY;
+    if (access_mode & ADIO_RDWR)
+       amode = amode | O_RDWR;
+    if (access_mode & ADIO_EXCL)
+       amode = amode | O_EXCL;
+   return amode;
+}
+
+
+
+int ad_plfs_hints(ADIO_File fd, int rank, char * hint){
+
+    int hint_value=0,flag,resultlen,mpi_ret;
+    char *value;
+    char err_buffer[MPI_MAX_ERROR_STRING];
+
+    // get the value of broadcast
+    value = (char *) ADIOI_Malloc((MPI_MAX_INFO_VAL+1));
+
+    mpi_ret=MPI_Info_get(fd->info,hint,MPI_MAX_INFO_VAL,value,&flag);
+
+    // If there is an error on the info get the rank and the error message
+    if(mpi_ret!=MPI_SUCCESS){
+        MPI_Error_string(mpi_ret,err_buffer,&resultlen);
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+        return -1;
+    }else{
+        if(flag) hint_value = atoi(value);
+    }
+    ADIOI_Free(value);
+    return hint_value;
+}
+
+void malloc_check(void *test_me,int rank){
+
+    if(!test_me){
+        plfs_debug("Rank %d failed a malloc check\n");
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+    }
+}
+
+void check_stream(int size,int rank){
+    
+    if(size<0){
+        plfs_debug("Rank %d had a stream with a negative return size\n");
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+    }
+}
--- /tmp/openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs.h	1969-12-31 17:00:00.000000000 -0700
+++ ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs.h	2011-01-31 16:39:35.000000000 -0700
@@ -0,0 +1,68 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/* 
+ *   $Id: ad_plfs.h,v 1.1 2010/11/29 19:59:01 adamm Exp $    
+ *
+ *   Copyright (C) 1997 University of Chicago. 
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#ifndef AD_PLFS_INCLUDE
+#define AD_PLFS_INCLUDE
+
+#ifndef ROMIOCONF_H_INCLUDED
+#include "romioconf.h"
+#define ROMIOCONF_H_INCLUDED
+#endif
+#ifdef ROMIO_PLFS_NEEDS_INT64_DEFINITION
+typedef long long int int64_t;
+#endif
+
+#include <unistd.h>
+#include <sys/types.h>
+#include <sys/uio.h>
+#include <fcntl.h>
+#include "plfs.h"
+#include "adio.h"
+
+
+void ADIOI_PLFS_Open(ADIO_File fd, int *error_code);
+void ADIOI_PLFS_Close(ADIO_File fd, int *error_code);
+void ADIOI_PLFS_ReadContig(ADIO_File fd, void *buf, int count, 
+                      MPI_Datatype datatype, int file_ptr_type,
+                     ADIO_Offset offset, ADIO_Status *status, int
+		     *error_code);
+void ADIOI_PLFS_WriteContig(ADIO_File fd, void *buf, int count, 
+                      MPI_Datatype datatype, int file_ptr_type,
+                      ADIO_Offset offset, ADIO_Status *status, int
+		      *error_code);   
+void ADIOI_PLFS_Fcntl(ADIO_File fd, int flag, ADIO_Fcntl_t *fcntl_struct, int
+		*error_code); 
+/*
+void ADIOI_PLFS_WriteStrided(ADIO_File fd, void *buf, int count,
+		       MPI_Datatype datatype, int file_ptr_type,
+		       ADIO_Offset offset, ADIO_Status *status, int
+		       *error_code);
+void ADIOI_PLFS_ReadStrided(ADIO_File fd, void *buf, int count,
+		       MPI_Datatype datatype, int file_ptr_type,
+		       ADIO_Offset offset, ADIO_Status *status, int
+		       *error_code);
+void ADIOI_PLFS_SetInfo(ADIO_File fd, MPI_Info users_info, int *error_code);
+*/
+void ADIOI_PLFS_Flush(ADIO_File fd, int *error_code);
+void ADIOI_PLFS_Delete(char *filename, int *error_code);
+void ADIOI_PLFS_Resize(ADIO_File fd, ADIO_Offset size, int *error_code);
+void ADIOI_PLFS_SetInfo(ADIO_File fd, MPI_Info users_info, int *error_code);
+
+int ad_plfs_amode( int access_mode );
+void malloc_check(void *test_me,int rank);
+void check_stream(int size,int rank);
+/* Check for hints passed from the command line
+ * Current hints
+ * plfs_enable_broadcast : Turn broadcast of index from root on
+ * plfs_compress_index   : Compress indexes before sending out
+ *                         useless if broadcast off
+ * plfs_flatten_close    : Flatten the index on the close
+ *
+ */    
+int ad_plfs_hints(ADIO_File fd, int rank, char *hint);
+#endif
--- /tmp/openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_close.c	1969-12-31 17:00:00.000000000 -0700
+++ ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_close.c	2011-01-31 16:39:35.000000000 -0700
@@ -0,0 +1,220 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/* 
+ *   $Id: ad_plfs_close.c,v 1.9 2004/10/04 15:51:01 robl Exp $    
+ *
+ *   Copyright (C) 1997 University of Chicago. 
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "ad_plfs.h"
+
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <unistd.h>
+
+int flatten_then_close(Plfs_fd *fd,int rank,int amode,int procs,
+            Plfs_close_opt *close_opt,const char *filename,uid_t);
+void check_error(int err,int rank);
+void reduce_meta(Plfs_fd *fd,const char * filename,Plfs_close_opt *close_opt);
+
+
+void ADIOI_PLFS_Close(ADIO_File fd, int *error_code)
+{
+    int err, rank, amode,procs;
+    static char myname[] = "ADIOI_PLFS_CLOSE";
+    uid_t uid = geteuid();
+    Plfs_close_opt close_opt;
+    close_opt.pinter=PLFS_MPIIO;
+    int flatten=0;
+    
+    plfs_debug("%s: begin\n", myname );
+
+    MPI_Comm_rank( MPI_COMM_WORLD, &rank );
+    MPI_Comm_size( MPI_COMM_WORLD, &procs);
+
+    amode = ad_plfs_amode( fd->access_mode );
+
+    if(fd->fs_ptr==NULL) {
+        // ADIO does a weird thing where it 
+        // passes CREAT to just 0 and then
+        // immediately closes.  When we handle
+        // the CREAT, we put a NULL in
+        *error_code = MPI_SUCCESS;
+        return;
+    }
+
+    flatten = ad_plfs_hints (fd , rank, "plfs_flatten_close"); 
+
+    // Only want to do this on write
+    double start_time,end_time;
+    start_time=MPI_Wtime();
+    if(flatten && fd->access_mode!=ADIO_RDONLY) {
+        // flatten on close takes care of calling plfs_close and setting
+        // up the close_opt
+        close_opt.valid_meta=0;
+        plfs_debug("Rank: %d in flatten then close\n",rank);
+        err = flatten_then_close(fd->fs_ptr, rank, amode, procs, &close_opt,
+                fd->filename,uid);
+    } else{
+        // for ADIO, just 0 creates the openhosts and the meta dropping 
+        // Grab the last offset and total bytes from all ranks and reduce to max
+        plfs_debug("Rank: %d in regular close\n",rank);
+        if(fd->access_mode!=ADIO_RDONLY){
+            reduce_meta(fd->fs_ptr,fd->filename,&close_opt);
+        }
+        err = plfs_close(fd->fs_ptr, rank, uid,amode,&close_opt);
+    }
+    plfs_debug("%d: close time: %.2f\n", rank,MPI_Wtime()-start_time);
+    
+    fd->fs_ptr = NULL;
+
+    if (err < 0 ) {
+	*error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+					   myname, __LINE__, MPI_ERR_IO,
+					   "**io",
+					   "**io %s", strerror(-err));
+    } else {
+         *error_code = MPI_SUCCESS;
+    }
+}
+
+
+int flatten_then_close(Plfs_fd *fd,int rank,int amode,int procs,
+        Plfs_close_opt *close_opt, const char *filename,uid_t uid)
+{
+    int index_size,err,index_total_size=0,streams_malloc=1,stop_buffer=0;
+    int *index_sizes,*index_disp;
+    char *index_stream,*index_streams;
+    double start_time,end_time;
+
+    // Get the index stream from the local index
+    index_size=plfs_index_stream(&(fd),&index_stream);
+    // Malloc space to receive all of the index sizes
+    // Do all procs need to do this? I think not
+    if(!rank) {
+        index_sizes=(int *)malloc(procs*sizeof(int));
+        if(!index_sizes) {
+            plfs_debug("Malloc failed:index size gather\n");
+            MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+        }
+    }
+
+    if(!rank) start_time=MPI_Wtime();
+    // Perform the gather of all index sizes to set up our vector call
+    MPI_Gather(&index_size,1,MPI_INT,index_sizes,1,MPI_INT,0,MPI_COMM_WORLD);
+    // Figure out how much space we need and then malloc if we are root
+    if(!rank){
+        end_time=MPI_Wtime();
+        plfs_debug("Gather of index sizes time:%.12f\n"
+                    ,end_time-start_time);
+        int count;
+        // Malloc space for out displacements
+        index_disp=malloc(procs*sizeof(int));
+        if(!index_disp){
+            plfs_debug("Displacements malloc has failed\n");
+            MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+        }
+        for(count=0;count<procs;count++){
+            index_disp[count]=index_total_size; 
+            // Calculate the size of the index
+            index_total_size+=index_sizes[count];
+            if(index_sizes[count]==-1) {
+                plfs_debug("Rank %d had an index that wasn't buffered\n",count);
+                stop_buffer=1;
+            }
+        }
+        plfs_debug("Total size of indexes %d\n",index_total_size);
+        if(!stop_buffer) {
+            index_streams=(char *)malloc((index_total_size*sizeof(char)));
+            if(!index_streams){
+                plfs_debug("Malloc failed:index streams\n");
+                streams_malloc=0;
+            }
+        }
+    }
+    
+    err=MPI_Bcast(&streams_malloc,1,MPI_INT,0,MPI_COMM_WORLD);
+    check_error(err,rank);
+
+    err=MPI_Bcast(&stop_buffer,1,MPI_INT,0,MPI_COMM_WORLD);
+    check_error(err,rank);
+    
+    
+    if(!rank) start_time=MPI_Wtime();
+    
+    // Gather all of the subindexes only if malloc succeeded
+    // and no one stopped buffering
+    if( streams_malloc && !stop_buffer){
+        MPI_Gatherv(index_stream,index_size,MPI_CHAR,index_streams,
+                    index_sizes,index_disp,MPI_CHAR,0,MPI_COMM_WORLD);
+    }
+
+    if(!rank) {
+        end_time=MPI_Wtime();
+        plfs_debug("Gatherv of indexes:%.12f\n"
+                    ,end_time-start_time);
+    }
+    // We are root lets combine all of our subindexes
+    if(!rank && streams_malloc && !stop_buffer){
+        plfs_debug("About to merge indexes for %s\n",filename);
+        start_time=MPI_Wtime();
+        plfs_merge_indexes(&(fd),index_streams,index_sizes,procs);
+        end_time=MPI_Wtime();
+        plfs_debug("Finished merging indexes time:%.12f\n"
+                    ,end_time-start_time);
+        start_time=MPI_Wtime();
+
+        plfs_flatten_index(fd,filename);
+        end_time=MPI_Wtime();
+        plfs_debug("Finished flattening time:%.12f\n"
+                    ,end_time-start_time);
+    }
+    
+    if(stop_buffer) reduce_meta(fd,filename,close_opt); 
+    // Close normally
+    // This should be fine before the previous if statement
+    err = plfs_close(fd, rank, uid, amode,close_opt);
+    
+    if(index_size>0) free(index_stream);
+    
+    if(!rank){
+        // Only root needs to complete these frees
+        free(index_sizes);
+        free(index_disp);
+        if(streams_malloc && !stop_buffer) free(index_streams);
+    }
+    // Everyone needs to free their index stream
+    // Root doesn't really need to make this call
+    // Could take out the plfs_index_stream call for root
+    // This is causing errors does the free to index streams clean this up?
+
+    return err;
+}
+
+void reduce_meta(Plfs_fd *fd,const char * filename,Plfs_close_opt *close_opt){
+    
+    int BLKSIZE=512;
+    struct stat buf;
+    size_t glbl_tot_byt=0;
+    int size_only=1;    // lazy stat
+    
+    plfs_getattr(fd,filename,&buf,size_only);
+    MPI_Reduce(&(buf.st_size),&(close_opt->last_offset),1,
+            MPI_LONG_LONG,MPI_MAX,0,MPI_COMM_WORLD);
+    MPI_Reduce(&(buf.st_blocks),&glbl_tot_byt,1,MPI_LONG_LONG,MPI_SUM,0,
+            MPI_COMM_WORLD); 
+    close_opt->total_bytes=glbl_tot_byt*BLKSIZE;
+    close_opt->valid_meta=1;
+}
+
+void check_error(int err,int rank){
+
+    if(err != MPI_SUCCESS){
+        int resultlen; 
+        char err_buffer[MPI_MAX_ERROR_STRING]; 
+        MPI_Error_string(err,err_buffer,&resultlen); 
+        printf("Error:%s | Rank:%d\n",err_buffer,rank); 
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO); 
+    }
+
+}
--- /tmp/openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_delete.c	1969-12-31 17:00:00.000000000 -0700
+++ ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_delete.c	2011-01-31 16:39:35.000000000 -0700
@@ -0,0 +1,26 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/* 
+ *   $Id: ad_plfs_delete.c,v 1.1 2010/11/29 19:59:01 adamm Exp $    
+ *
+ *   Copyright (C) 1997 University of Chicago. 
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "ad_plfs.h"
+#include "adio.h"
+
+void ADIOI_PLFS_Delete(char *filename, int *error_code)
+{
+    int err;
+    static char myname[] = "ADIOI_PLFS_DELETE";
+    plfs_debug("%s: begin\n", myname );
+
+    err = plfs_unlink(filename);
+    if (err < 0) {
+	*error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+					   myname, __LINE__, MPI_ERR_IO,
+					   "**io",
+					   "**io %s", strerror(-err));
+    }
+    else *error_code = MPI_SUCCESS;
+}
--- /tmp/openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_fcntl.c	1969-12-31 17:00:00.000000000 -0700
+++ ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_fcntl.c	2011-01-31 16:39:35.000000000 -0700
@@ -0,0 +1,49 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/* 
+ *
+ *   Copyright (C) 1997 University of Chicago. 
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "ad_plfs.h"
+
+void ADIOI_PLFS_Fcntl(ADIO_File fd, int flag, ADIO_Fcntl_t *fcntl_struct,
+		      int *error_code)
+{
+    static char myname[] = "ADIOI_PVFS_FCNTL";
+    struct stat buf;
+    int ret, size_only;
+
+    plfs_debug( "%s: begin\n", myname );
+
+    switch(flag) {
+    case ADIO_FCNTL_GET_FSIZE:
+        size_only = 1;  // like a lazy stat or stat-lite
+        ret = plfs_getattr( fd->fs_ptr, fd->filename, &buf, size_only );
+        if ( ret == 0 ) {
+            fcntl_struct->fsize = buf.st_size;
+            *error_code = MPI_SUCCESS;
+        } else {
+	    *error_code = MPIO_Err_create_code(MPI_SUCCESS,
+					       MPIR_ERR_RECOVERABLE, myname,
+					       __LINE__, MPI_ERR_IO, "**io",
+					       "**io %s", strerror(errno));
+        }
+	//if (fd->fp_sys_posn != -1) {
+	//     pvfs_lseek64(fd->fd_sys, fd->fp_sys_posn, SEEK_SET);
+        //}
+	break;
+
+    case ADIO_FCNTL_SET_DISKSPACE:
+    case ADIO_FCNTL_SET_ATOMICITY:
+    default:
+	/* --BEGIN ERROR HANDLING-- */
+	*error_code = MPIO_Err_create_code(MPI_SUCCESS,
+					   MPIR_ERR_RECOVERABLE,
+					   myname, __LINE__,
+					   MPI_ERR_ARG,
+					   "**flag", "**flag %d", flag);
+	return;  
+	/* --END ERROR HANDLING-- */
+    }
+}
--- /tmp/openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_flush.c	1969-12-31 17:00:00.000000000 -0700
+++ ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_flush.c	2011-01-31 16:39:35.000000000 -0700
@@ -0,0 +1,31 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/* 
+ *   $Id: ad_plfs_flush.c,v 1.1 2010/11/29 19:59:01 adamm Exp $    
+ *
+ *   Copyright (C) 1997 University of Chicago. 
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "ad_plfs.h"
+
+void ADIOI_PLFS_Flush(ADIO_File fd, int *error_code)
+{
+    int err, rank;
+    static char myname[] = "ADIOI_PLFS_FLUSH";
+    plfs_debug( "%s: begin\n", myname );
+
+    MPI_Comm_rank(fd->comm, &rank);
+
+    // even though this is a collective routine, everyone must flush here
+    // because everyone has there own data file handle
+    err = plfs_sync(fd->fs_ptr,rank);
+
+    if (err < 0) {
+	*error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+					   myname, __LINE__, MPI_ERR_IO,
+					   "**io",
+					   "**io %s", strerror(-err));
+    } else {
+         *error_code = MPI_SUCCESS;
+    }
+}
--- /tmp/openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_hints.c	1969-12-31 17:00:00.000000000 -0700
+++ ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_hints.c	2011-01-31 16:39:35.000000000 -0700
@@ -0,0 +1,97 @@
+#include "ad_plfs.h"
+
+#define POORMANS_GDB \
+    printf("%d in %s:%d\n", rank, __FUNCTION__,__LINE__);
+
+void ADIOI_PLFS_SetInfo(ADIO_File fd, MPI_Info users_info, int *error_code) {
+    static char myname[] = "ADIOI_PLFS_SETINFO";
+    char* value;
+    int flag, tmp_val = -1;
+    int disable_broadcast = 1;
+    int compress_index = 0;
+    int flatten_close = 0;
+    int parindex_read = 0;
+    int gen_error_code,rank;
+
+    MPI_Comm_rank( fd->comm, &rank );
+    *error_code = MPI_SUCCESS;
+    
+    if ((fd->info) == MPI_INFO_NULL) {
+	    /* This must be part of the open call. can set striping parameters 
+         * if necessary. 
+         */ 
+	    MPI_Info_create(&(fd->info));
+
+        if (users_info != MPI_INFO_NULL) {
+	        value = (char *) ADIOI_Malloc((MPI_MAX_INFO_VAL+1)*sizeof(char));
+            /* plfs_disable_broadcast */ 
+            MPI_Info_get(users_info, "plfs_disable_broadcast", MPI_MAX_INFO_VAL,
+                            value, &flag);
+            if (flag) {
+                disable_broadcast = atoi(value);
+                tmp_val = disable_broadcast;
+                MPI_Bcast(&tmp_val, 1, MPI_INT, 0, fd->comm);
+                if (tmp_val != disable_broadcast) {
+                    FPRINTF(stderr, "ADIOI_PLFS_SetInfo: "
+                            "the value for key \"plfs_disable_broadcast\" "
+                            "must be the same on all processes\n");
+                    MPI_Abort(MPI_COMM_WORLD, 1);
+                }
+	            MPI_Info_set(fd->info, "plfs_disable_broadcast", value); 
+            }
+            MPI_Info_get(users_info, "plfs_compress_index", MPI_MAX_INFO_VAL,
+                            value, &flag);
+            /* Compression flag*/
+            if(flag){
+                compress_index = atoi(value);
+                tmp_val = compress_index;
+                MPI_Bcast(&tmp_val,1,MPI_INT,0,fd->comm);
+                if (tmp_val != compress_index) {
+                    FPRINTF(stderr, "ADIOI_PLFS_SetInfo: "
+                            "the value for key \"plfs_compress_index\" "
+                            "must be the same on all processes\n");
+                    MPI_Abort(MPI_COMM_WORLD, 1);
+                }
+	            MPI_Info_set(fd->info, "plfs_compress_index", value); 
+            }
+            /* flatten_close */
+            MPI_Info_get(users_info, "plfs_flatten_close", MPI_MAX_INFO_VAL,
+                            value, &flag);
+            if(flag){
+                flatten_close = atoi(value);
+                tmp_val = flatten_close;
+                MPI_Bcast(&tmp_val,1,MPI_INT,0,fd->comm);
+                if (tmp_val != flatten_close) {
+                    FPRINTF(stderr, "ADIOI_PLFS_SetInfo: "
+                            "the value for key \"plfs_flatten_close\" "
+                            "must be the same on all processes\n");
+                    MPI_Abort(MPI_COMM_WORLD, 1);
+                }
+	            MPI_Info_set(fd->info, "plfs_flatten_close", value); 
+            }
+            /* Parallel Index Read  */
+            MPI_Info_get(users_info, "plfs_parindex_read", MPI_MAX_INFO_VAL,
+                            value, &flag);
+            if(flag){
+                parindex_read = atoi(value);
+                tmp_val = parindex_read;
+                MPI_Bcast(&tmp_val,1,MPI_INT,0,fd->comm);
+                if (tmp_val != parindex_read) {
+                    FPRINTF(stderr, "ADIOI_PLFS_SetInfo: "
+                            "the value for key \"plfs_parindex_read\" "
+                            "must be the same on all processes\n");
+                    MPI_Abort(MPI_COMM_WORLD, 1);
+                }
+	            MPI_Info_set(fd->info, "plfs_parindex_read", value); 
+            }
+	        ADIOI_Free(value);
+        }
+    }
+
+    ADIOI_GEN_SetInfo(fd, users_info, &gen_error_code); 
+    /* If this function is successful, use the error code 
+     * returned from ADIOI_GEN_SetInfo
+     * otherwise use the error_code generated by this function
+     */
+    if(*error_code == MPI_SUCCESS) *error_code = gen_error_code;
+}
--- /tmp/openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_open.c	1969-12-31 17:00:00.000000000 -0700
+++ ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_open.c	2011-01-31 16:39:35.000000000 -0700
@@ -0,0 +1,735 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/* 
+ *   $Id: ad_plfs_open.c,v 1.1 2010/11/29 19:59:01 adamm Exp $    
+ *
+ *   Copyright (C) 1997 University of Chicago. 
+ *   See COPYRIGHT notice in top-level directory.
+ */
+#include "ad_plfs.h"
+#include "zlib.h"
+#include <dirent.h>
+#include <string.h>
+
+#define VERBOSE_DEBUG 0
+#define BIT_ARRAY_LENGTH 1024
+#define HOSTDIR_PREFIX "hostdir."
+#define HOSTDIR_PREFIX_LEN 8  
+
+// A bitmap to hold the number of and id of 
+// hostdirs inside of the container
+typedef struct {
+    unsigned int bit:1;
+}Bit;
+Bit bitmap[BIT_ARRAY_LENGTH]={0};
+
+// a bunch of helper macros we added when we had a really hard time debugging
+// this file.  We were confused by ADIO calling rank 0 initially on the create
+// and then again on the open (and a bunch of other stuff)
+#if VERBOSE_DEBUG == 1
+    #define POORMANS_GDB \
+        fprintf(stderr,"%d in %s:%d\n", rank, __FUNCTION__,__LINE__);
+
+    #define TEST_BCAST(X) \
+    {\
+        int test = -X;\
+        if(rank==0) test = X; \
+        MPIBCAST( &test, 1, MPI_INT, 0, MPI_COMM_WORLD );\
+        fprintf(stderr,"rank %d got test %d\n",rank,test);\
+        if(test!=X){ \
+            MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);\
+        }\
+    }
+#else
+    #define POORMANS_GDB {}
+    #define TEST_BCAST(X) {}
+#endif
+
+#define MPIBCAST(A,B,C,D,E) \
+    POORMANS_GDB \
+    { \
+        ret = MPI_Bcast(A,B,C,D,E); \
+        if(ret!=MPI_SUCCESS) { \
+            int resultlen; \
+            char err_buffer[MPI_MAX_ERROR_STRING]; \
+            MPI_Error_string(ret,err_buffer,&resultlen); \
+            printf("Error:%s | Rank:%d\n",err_buffer,rank); \
+            MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO); \
+        } \
+    } \
+    POORMANS_GDB
+    
+#define MPIALLGATHER(A,B,C,D,E,F,G)\
+{\
+    ret = MPI_Allgather(A,B,C,D,E,F,G);\
+    if(ret!= MPI_SUCCESS){\
+        int resultlen; \
+        char err_buffer[MPI_MAX_ERROR_STRING]; \
+        MPI_Error_string(ret,err_buffer,&resultlen); \
+        printf("Error:%s | Rank:%d\n",err_buffer,rank); \
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO); \
+    } \
+} \
+
+#define MPIALLGATHERV(A,B,C,D,E,F,G,H)\
+{\
+    ret = MPI_Allgatherv(A,B,C,D,E,F,G,H);\
+    if(ret!= MPI_SUCCESS){\
+        int resultlen; \
+        char err_buffer[MPI_MAX_ERROR_STRING]; \
+        MPI_Error_string(ret,err_buffer,&resultlen); \
+        printf("Error:%s | Rank:%d\n",err_buffer,rank); \
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO); \
+    } \
+}  
+
+#define MPIGATHER(A,B,C,D,E,F,G,H)\
+{\
+    ret = MPI_Gather(A,B,C,D,E,F,G,H);\
+    if(ret!= MPI_SUCCESS){\
+        int resultlen; \
+        char err_buffer[MPI_MAX_ERROR_STRING]; \
+        MPI_Error_string(ret,err_buffer,&resultlen); \
+        printf("Error:%s | Rank:%d\n",err_buffer,rank); \
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO); \
+    } \
+}
+
+#define MPIGATHERV(A,B,C,D,E,F,G,H,I)\
+{\
+    ret = MPI_Gatherv(A,B,C,D,E,F,G,H,I);\
+    if(ret!= MPI_SUCCESS){\
+        int resultlen; \
+        char err_buffer[MPI_MAX_ERROR_STRING]; \
+        MPI_Error_string(ret,err_buffer,&resultlen); \
+        printf("Error:%s | Rank:%d\n",err_buffer,rank); \
+        MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO); \
+    } \
+}
+
+
+int open_helper(ADIO_File fd,Plfs_fd **pfd,int *error_code,int perm, 
+        int amode,int rank);
+int broadcast_index(Plfs_fd **pfd, ADIO_File fd, 
+        int *error_code,int perm,int amode,int rank,int compress_flag);
+int getPerm(ADIO_File);
+
+int getPerm(ADIO_File fd) {
+    int perm = fd->perm;
+    if (fd->perm == ADIO_PERM_NULL) {
+        int old_mask = umask(022);
+        umask(old_mask);
+        perm = old_mask ^ 0666;
+    }
+    return perm;
+}
+// Par index read stuff
+int par_index_read(ADIO_File fd,Plfs_fd **pfd,int *error_code,int perm,
+        int amode,int rank, MPI_Comm openers_comm,void **global_index);
+// Fills in the bitmap structure
+int num_host_dirs(int *hostdir_count,char *target);
+// Printer for the bitmap struct
+void host_list_print(Bit *bitmap);
+// Function to calculate the extra ranks
+int extra_rank_calc(int np,int num_host_dir);
+// Number of ranks per comm
+int ranks_per_comm_calc(int np,int num_host_dir);
+// Based on my rank how many ranks are in my hostdir comm
+int rank_to_size(int rank,int ranks_per_comm,int extra_rank,
+                            int np,int group_index);
+// Index used for a color that determines my hostdir comm
+int rank_to_group_index(int rank,int ranks_per_comm,int extra_rank);
+// Converts bitmap position to a dirname
+char* bitmap_to_dirname(Bit *bitmap,int group_index,
+        char *target,int mult,int np);
+// Called when num procs >= num_host_dirs
+void split_and_merge(MPI_Comm openers_comm,int rank,int extra_rank,
+        int ranks_per_comm,int np,char *filename,void **global_index);
+// Called when num hostdirs > num procs
+void read_and_merge(MPI_Comm comm,int rank,
+        int np,int hostdir_per_rank,char *filename,void **global_index);
+// Added to handle the case where one rank must read more than one hostdir
+char *count_to_hostdir(Bit *bitmap,int stop_point,int *count,
+        int *hostdir_found,char *filename,char *target,int first);
+// Broadcast the bitmap to interested parties
+void bcast_bitmap(MPI_Comm comm,int rank);
+
+void ADIOI_PLFS_Open(ADIO_File fd, int *error_code)
+{
+    Plfs_fd *pfd =NULL;
+    // I think perm is the mode and amode is the flags
+    int err = 0,perm, amode, old_mask,rank,ret;
+ 
+    MPI_Comm_rank( MPI_COMM_WORLD, &rank );
+    static char myname[] = "ADIOI_PLFS_OPEN";
+
+    perm = getPerm(fd);
+    amode = ad_plfs_amode(fd->access_mode);
+
+    // ADIO makes 2 calls into here:
+    // first, just 0 with CREATE
+    // then everyone without
+    if (fd->access_mode & ADIO_CREATE) {
+        err = plfs_create(fd->filename, perm, amode, rank);
+        if ( err != 0 ) {
+            *error_code =MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+					   myname, __LINE__, MPI_ERR_IO,
+					   "**io",
+					   "**io %s", strerror(-err));
+        } else {
+            *error_code = MPI_SUCCESS;
+        }
+        fd->fs_ptr = NULL; // set null because ADIO is about to close it
+        return;
+    }
+    
+    // if we make it here, we're doing RDONLY, WRONLY, or RDWR
+    err=open_helper(fd,&pfd,error_code,perm,amode,rank);
+    MPIBCAST( &err, 1, MPI_INT, 0, MPI_COMM_WORLD );
+    if ( err != 0 ) {
+        *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+					   myname, __LINE__, MPI_ERR_IO,
+					   "**io",
+					   "**io %s", strerror(-err));
+        plfs_debug( "%s: failure\n", myname );
+        return;
+    } else {
+        plfs_debug( "%s: Success on open (%d)!\n", myname, rank );
+        *error_code = MPI_SUCCESS;
+    }
+    return;
+}
+
+// a helper that determines whether 0 distributes the index to everyone else
+// or whether everyone just calls plfs_open directly
+int open_helper(ADIO_File fd,Plfs_fd **pfd,int *error_code,int perm,
+        int amode,int rank)
+{
+    int err = 0, disabl_broadcast=0, compress_flag=0,close_flatten=0;
+    int parallel_index_read=0;
+    static char myname[] = "ADIOI_PLFS_OPENHELPER";
+    Plfs_open_opt open_opt;
+    
+    if (fd->access_mode==ADIO_RDONLY) {
+            disabl_broadcast = ad_plfs_hints(fd,rank,"plfs_disable_broadcast");
+            compress_flag = ad_plfs_hints(fd,rank,"plfs_compress_index");
+            parallel_index_read = ad_plfs_hints(fd,rank,"plfs_parindex_read");
+            plfs_debug("Disable_bcast:%d,compress_flag:%d\n",
+                        disabl_broadcast,compress_flag);
+            // I took out the extra broadcasts at this point. ad_plfs_hints 
+            // has code to make sure that all ranks have the same value
+            // for the hint
+    } else {
+        disabl_broadcast = 1; // we don't create an index unless we're in read mode
+        compress_flag=0;
+    }
+    // This is new code added to handle the parallel_index_read case
+    if( (fd->access_mode==ADIO_RDONLY) && parallel_index_read){
+        void *global_index;
+        // Function to start the parallel index read
+        par_index_read(fd,pfd,error_code,perm,amode,rank,
+                MPI_COMM_WORLD,&global_index);
+        open_opt.pinter = PLFS_MPIIO;
+        open_opt.index_stream=global_index;
+        err = plfs_open(pfd,fd->filename,amode,rank,perm,&open_opt);
+        free(global_index);
+    }
+    // If we are read only and broadcast isn't disabled let's broadcast that index
+    else if(!disabl_broadcast){
+        err = broadcast_index(pfd,fd,error_code,perm,amode,rank,compress_flag);
+    } else {
+        open_opt.pinter = PLFS_MPIIO;
+        open_opt.index_stream=NULL;
+        close_flatten = ad_plfs_hints(fd,rank,"plfs_flatten_close");
+        // Let's only buffer when the flatten on close hint is passed
+        // and we are in WRONLY mode
+        open_opt.buffer_index=close_flatten;
+        plfs_debug("Opening without a broadcast\n");
+        // everyone opens themselves (write mode or read mode w/out broacast)
+        err = plfs_open( pfd, fd->filename, amode, rank, perm ,&open_opt);
+    }
+    
+    if ( err < 0 ) {
+        *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+					   myname, __LINE__, MPI_ERR_IO,
+					   "**io",
+					   "**io %s", strerror(-err));
+        plfs_debug( "%s: failure\n", myname );
+        return -1;
+    } else {
+        plfs_debug( "%s: Success on open(%d)!\n", myname, rank );
+        fd->fs_ptr = *pfd;
+        fd->fd_direct = -1;
+        *error_code = MPI_SUCCESS;
+        return 0;
+    }
+}
+
+// 0 gets the index by calling plfs_open() first and then extracting the index
+// it then broadcasts that to the rest who then pass it to their own plfs_open()
+int broadcast_index(Plfs_fd **pfd, ADIO_File fd, 
+        int *error_code,int perm,int amode,int rank,int compress_flag) 
+{
+    int err = 0,ret;
+    char *index_stream;
+    char *compr_index;
+    // [0] is index stream size [1] is compressed size
+    unsigned long index_size[2]={0};
+    Plfs_open_opt open_opt;
+    open_opt.pinter = PLFS_MPIIO; 
+    open_opt.index_stream=NULL;
+    open_opt.buffer_index=0;
+    if(rank==0){ 
+        err = plfs_open(pfd, fd->filename, amode, rank, perm , &open_opt);
+    }
+    MPIBCAST(&err,1,MPI_INT,0,MPI_COMM_WORLD);   // was 0's open successful?
+    if(err !=0 ) return err;
+
+
+    // rank 0 turns the index into a stream, broadcasts its size, then it
+    if(rank==0){
+        plfs_debug("In broadcast index with compress_flag:%d\n",compress_flag);
+        index_size[0] = index_size[1] = plfs_index_stream(pfd,&index_stream); 
+        if(index_size[0]<0) MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+        
+        if(compress_flag){
+            plfs_debug("About to malloc the compressed index space\n");
+            compr_index=malloc(index_size[0]);
+            // Check the malloc
+            if(!compr_index){
+                plfs_debug("Rank %d aborting because of failed malloc\n",rank);
+                MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+            }
+            plfs_debug("About to compress the index\n");
+            // Check the compress
+            if(compress(compr_index,&index_size[1],index_stream,index_size[0])
+                    !=Z_OK)
+            {
+                plfs_debug("Compression of index has failed\n");
+                MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+            }
+
+        }
+    }
+    // Original index stream size
+    if (!rank) 
+        plfs_debug("Broadcasting the sizes of the index:%d "
+                "and compressed index%d\n" ,index_size[0],index_size[1]);
+    
+    MPIBCAST(index_size, 2, MPI_LONG, 0, MPI_COMM_WORLD);
+   
+    if(rank!=0) {
+        index_stream = malloc(index_size[0]);
+        if(compress_flag) {
+            compr_index = malloc(index_size[1]);
+            if(!compr_index ){
+                plfs_debug("Rank %d aborting because of failed malloc\n",rank);
+                MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+            }
+        }
+        //We need to check that the malloc succeeded or the broadcast is in vain
+        if(!index_stream ){
+            plfs_debug("Rank %d aborting because of a failed malloc\n",rank);
+            MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+        }
+    }
+    
+    if (compress_flag) {
+        if(!rank) plfs_debug("Broadcasting compressed index\n");
+        MPIBCAST(compr_index,index_size[1],MPI_CHAR,0,MPI_COMM_WORLD);
+    }else{
+        if(!rank) plfs_debug("Broadcasting full index\n");
+        MPIBCAST(index_stream,index_size[0],MPI_CHAR,0,MPI_COMM_WORLD);
+    }
+    // Broadcast compressed index
+    if(rank!=0) {
+        unsigned long uncompr_len=index_size[0];
+        // Uncompress the index
+        if(compress_flag) {
+            plfs_debug("Rank: %d has compr_len %d and expected expanded of %d\n"
+                    ,rank,index_size[1],uncompr_len);
+            int ret=uncompress(index_stream, 
+                    &uncompr_len,compr_index,index_size[1]);
+        
+            if(ret!=Z_OK)
+            {
+                plfs_debug("Rank %d aborting bec failed uncompress\n",rank);
+                if(ret==Z_MEM_ERROR) plfs_debug("Mem error\n");
+                if(ret==Z_BUF_ERROR) plfs_debug("Buffer error\n");
+                if(ret==Z_DATA_ERROR) plfs_debug("Data error\n");
+                MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+            }
+        
+            // Error if the uncompressed length doesn't match original length 
+            if(uncompr_len!=index_size[0]){
+                plfs_debug("Uncompressed len != original index size\n");
+                MPI_Abort(MPI_COMM_WORLD,MPI_ERR_IO);
+            }
+        }
+        open_opt.index_stream=index_stream;
+        err = plfs_open(pfd,fd->filename,amode,rank,perm,&open_opt);
+    }
+    if(compress_flag) free(compr_index);
+    free(index_stream);
+    return 0;
+} 
+
+int par_index_read(ADIO_File fd,Plfs_fd **pfd,int *error_code,int perm,
+        int amode,int rank, MPI_Comm openers_comm,void **global_index){
+
+    // Each rank and the number of processes playing
+    int np,extra_rank,ret;
+    MPI_Comm_size(openers_comm, &np);
+    // Rank 0 reads the top level directory and sets the
+    // next two variables
+    int num_host_dir=0;
+
+    // Every other rank can figures this out using num_host_dir
+    int ranks_per_comm=0;
+    // Only used is ranks per comm equals zero
+    int hostdir_per_rank=0;
+    char * filename;
+
+
+    plfs_expand_path(fd->filename,&filename);
+    // Rank 0 only code
+    if(!rank){
+        // Find out how many hostdirs we currently have 
+        // and save info in a bitmap 
+        num_host_dirs(&num_host_dir,filename);
+    }
+    plfs_debug("Num of hostdirs calculated is |%d|\n",num_host_dir); 
+    // Bcast usage brought down by the MPI_Comm_split
+    // Was using MPI_Group_incl which needed an array of 
+    // all members of the group. Don't forget to plan and 
+    // look at available methods to get the job done
+    MPIBCAST(&num_host_dir,1,MPI_INT,0,openers_comm);
+    
+    // Get some information used to determine the group index 
+    ranks_per_comm=ranks_per_comm_calc(np,num_host_dir);
+    // Split based on the number of ranks per comm. If zero we 
+    // take another path and the extra_rank and hostdir_per_rank
+    // calculation are different
+    if(ranks_per_comm) extra_rank=extra_rank_calc(np,num_host_dir); 
+    if(!ranks_per_comm) {
+        extra_rank=num_host_dir-np;
+        hostdir_per_rank=num_host_dir/np;
+        int left_over=num_host_dir%np;
+        if(rank<left_over){
+            hostdir_per_rank++;
+        }
+    }
+    // Here we split on the ranks per comm. Should not be necessary
+    // to check return values. Functions will abort if an error is encountered
+    if(ranks_per_comm) split_and_merge(openers_comm,rank,extra_rank,
+            ranks_per_comm,np,filename,global_index);
+    if(!ranks_per_comm) read_and_merge(openers_comm,rank,np,hostdir_per_rank,
+            filename,global_index); 
+    return 0; 
+}
+
+// This is the case where a rank has to read more than one hostdir
+void read_and_merge(MPI_Comm comm,int rank,
+        int np,int hostdir_per_rank,char *filename,void **global_index)
+{
+    char *targets;
+    int index_sz,ret,count,*index_sizes,*index_disp,index_total_size=0;
+    int global_index_sz;
+    void *index_stream,*index_streams;
+
+    // Get the bitmap
+    bcast_bitmap(comm,rank);
+    // Figure out which hostdirs I have to read
+    targets=bitmap_to_dirname(bitmap,rank,filename,
+            hostdir_per_rank,np);
+    // Read the hostdirs and return an index stream
+    index_sz=plfs_hostdir_rddir(&index_stream,targets,rank,filename);
+    // Make sure it was malloced 
+    check_stream(index_sz,rank);
+    // Targets no longer needed
+    free(targets);
+    // Used to hold the sizes of indexes from all procs 
+    // needed for ALLGATHERV
+    index_sizes=malloc(sizeof(int)*np);
+    malloc_check((void *)index_sizes,rank);
+    // Gets the index sizes from all ranks
+    MPIALLGATHER(&index_sz,1,MPI_INT,index_sizes,
+            1,MPI_INT,comm);
+    // Set up displacements
+    index_disp=malloc(sizeof(int)*np);
+    malloc_check((void *)index_disp,rank);
+    for(count=0;count<np;count++){
+        index_disp[count]=index_total_size;
+        index_total_size+=index_sizes[count];
+    }
+    // Holds all of the index streams from all procs involved in the process
+    index_streams=malloc(sizeof(char)*index_total_size);
+    malloc_check(index_streams,rank);
+    // ALLGATHERV grabs all of the indexes from all the procs
+    MPIALLGATHERV(index_stream,index_sz,MPI_CHAR,index_streams,index_sizes,
+            index_disp,MPI_CHAR,comm);
+    // Merge all of the stream that we were passed to get a global index
+    global_index_sz=plfs_parindexread_merge(filename,index_streams,
+            index_sizes,np,global_index);
+    check_stream(global_index_sz,rank);
+    plfs_debug("Rank |%d| global size |%d|\n",rank,global_index_sz);
+    free(index_streams);
+}
+
+void bcast_bitmap(MPI_Comm comm, int rank){
+    int ret;
+    int bitmap_bcast_sz = BIT_ARRAY_LENGTH/sizeof(MPI_UNSIGNED_CHAR);
+    MPIBCAST(bitmap,bitmap_bcast_sz,MPI_UNSIGNED_CHAR,0,comm);
+}
+
+// If ranks > hostdirs we can split up our comm 
+void split_and_merge(MPI_Comm openers_comm,int rank,int extra_rank,
+        int ranks_per_comm,int np,char *filename,void **global_index)
+{
+    
+    int new_rank,color,group_index,hc_sz,ret,buf_sz=0;
+    int *index_sizes,*index_disp,count,index_total_size=0;
+    char *index_files, *index_streams;
+    void *index_stream;
+
+    MPI_Comm hostdir_comm,hostdir_zeros_comm;
+    // Group index is the color 
+    group_index = rank_to_group_index(rank,ranks_per_comm,extra_rank);
+    // Split the world communicator
+    MPI_Comm_split(openers_comm,group_index,rank,&hostdir_comm);
+    MPI_Comm_size(hostdir_comm,&hc_sz);
+    // Grab our rank within the hostdir communicator
+    MPI_Comm_rank (hostdir_comm, &new_rank);
+   
+    // Get a color for a communicator between all rank 0's in a hostdir comm
+    if(!new_rank) color = 1;
+    else color = MPI_UNDEFINED;
+    // The split for hostdir zeros comm
+    MPI_Comm_split(openers_comm,color,rank,&hostdir_zeros_comm);
+    // Hostdir zeros
+    if(!new_rank) {
+        char *fn_ptr;
+        // Broadcast the bitmap to the leaders
+        bcast_bitmap(hostdir_zeros_comm,rank);
+        // Convert my group index into the dir I should read
+        fn_ptr= bitmap_to_dirname(bitmap,group_index,filename,0,np);
+        // Hostdir zero reads the hostdir and converts into a list
+        buf_sz=plfs_hostdir_zero_rddir((void **)&index_files,fn_ptr,rank);
+        check_stream(buf_sz,rank);
+        free(fn_ptr);
+    }
+    // Send the size of the hostdir file list
+    MPIBCAST(&buf_sz,1,MPI_INT,0,hostdir_comm);
+    // Get space for the hostdir file list
+    if(new_rank){
+        index_files=malloc(sizeof(MPI_CHAR)*buf_sz);
+        malloc_check(index_files,rank);
+    }
+    // Get the hostdir file list
+    MPIBCAST(index_files,buf_sz,MPI_CHAR,0,hostdir_comm);
+    // Take the hostdir file list and convert to an index stream
+    buf_sz=plfs_parindex_read(new_rank,hc_sz,index_files,&index_stream,
+            filename);
+    check_stream(buf_sz,rank);
+    free(index_files);
+    if(!new_rank){
+        index_sizes=malloc(sizeof(int)*hc_sz);
+        malloc_check((void *)index_sizes,rank);
+    }
+    // Make sure hostdir rank 0 knows how much index data to expect
+    MPIGATHER(&buf_sz,1,MPI_INT,index_sizes,1,
+            MPI_INT,0,hostdir_comm);
+    // Set up displacements
+    if(!new_rank){
+        index_disp=malloc(sizeof(int)*hc_sz);
+        malloc_check((void *)index_disp,rank);
+        for(count=0;count<hc_sz;count++){
+            // Displacements
+            index_disp[count]=index_total_size;
+            index_total_size+=index_sizes[count];
+        }
+        // Space for the index streams from my hostdir comm
+        index_streams=malloc(sizeof(char)*index_total_size);
+        malloc_check(index_stream,rank);
+    }
+    // Gather the index streams from your hostdir
+    MPIGATHERV(index_stream,buf_sz,MPI_CHAR,index_streams,index_sizes,
+                index_disp,MPI_CHAR,0,hostdir_comm);
+    void *hostdir_index_stream;
+    int hostdir_index_sz;
+    // Hostdir leader
+    if(!new_rank){
+        // Merge the indexes that were gathered
+        hostdir_index_sz=plfs_parindexread_merge(filename,index_streams,
+            index_sizes,hc_sz,&hostdir_index_stream);
+        free(index_disp);
+        free(index_sizes);
+    }
+    // No longer need this information
+    free(index_stream);
+    int hzc_size,global_index_sz;
+    // Hostdir leader pass information to the other leaders
+    if(!new_rank) {
+        index_total_size=0;
+        MPI_Comm_size(hostdir_zeros_comm,&hzc_size);
+        // Store the sizes of all the leaders index streams
+        index_sizes=malloc(sizeof(int)*hzc_size);
+        malloc_check((void *)index_sizes,rank);
+        // Grab these sizes
+        MPIALLGATHER(&hostdir_index_sz,1,MPI_INT,index_sizes,
+            1,MPI_INT,hostdir_zeros_comm);
+        // Set up for GatherV
+        index_disp=malloc(sizeof(int)*hzc_size);
+        malloc_check((void *) index_disp,rank);
+        for(count=0;count<hzc_size;count++){
+            // Displacements
+            index_disp[count]=index_total_size;
+            index_total_size+=index_sizes[count];
+        }
+        index_streams=malloc(sizeof(char)*index_total_size);
+        malloc_check(index_streams,rank);
+        // Receive the index streams from all hotdir zeros
+        MPIALLGATHERV(hostdir_index_stream,hostdir_index_sz,MPI_CHAR,
+            index_streams,index_sizes,index_disp,MPI_CHAR,hostdir_zeros_comm);
+        // Merge these streams into a single global index
+        global_index_sz=plfs_parindexread_merge(filename,index_streams,
+                index_sizes,hzc_size,global_index);
+        check_stream(global_index_sz,rank);
+        // Free all of our malloced structures
+        free(index_streams);
+        free(hostdir_index_stream);
+        free(index_disp);
+        free(index_sizes);
+    }
+    // Get the size of the global index
+    MPIBCAST(&global_index_sz,1,MPI_INT,0,hostdir_comm);
+    // Malloc space if we are not hostdir leaders
+    if(new_rank) *global_index=malloc(sizeof(char)*global_index_sz);
+    malloc_check(global_index,rank);
+    //Hostdir leaders broadcast the global index
+    MPIBCAST(*global_index,global_index_sz,MPI_CHAR,0,hostdir_comm);
+    // Don't forget to  call Comm free  on created comms before MPI_Finalize 
+    // or you will encounter problems
+    MPI_Comm_free(&hostdir_comm);
+    if(!new_rank) {
+        MPI_Comm_free(&hostdir_zeros_comm);
+    }
+}
+
+// Simple print function
+void host_list_print(Bit* bitmap){
+    int count;
+    for(count=0;count<BIT_ARRAY_LENGTH;count++){
+        if(bitmap[count].bit==1) printf("Hostdir at position %d\n",count);
+    }
+}
+
+// Function that reads in the hostdirs and sets the bitmap 
+int num_host_dirs(int *hostdir_count,char *target){
+    // Directory reading variables
+    DIR *dirp;
+    struct dirent *dirent;
+
+    // Open the directory and check value
+    if((dirp=opendir(target)) == NULL){
+        plfs_debug("Num hostdir opendir error on %s\n",target);
+        return -1;
+    }
+
+    // Start reading the directory
+    while(dirent = readdir(dirp) ){
+        // Look for entries that beging with hostdir
+        if(strncmp("hostdir.",dirent->d_name,8)==0){
+            char * substr;
+            substr=strtok(dirent->d_name,".");
+            substr=strtok(NULL,".");
+            int index = atoi(substr);
+            bitmap[index].bit=1;
+            (*hostdir_count)++;
+        }
+    }
+    // Close the dir error out if we have a problem
+    if (closedir(dirp) == -1 ){
+        plfs_debug("Num hostdir closedir error on %s\n",target);
+        return -1;
+    }
+}
+
+// Calculates the number of ranks per communication group
+// Split comm makes this many subgroups
+int ranks_per_comm_calc(int np,int num_host_dir){
+    return np/num_host_dir;
+}
+
+// Get the amount of left over ranks, our values
+// are not going to divide evenly
+int extra_rank_calc(int np,int num_host_dir){
+    return  np%num_host_dir;
+}
+
+// Using the rank get the index/color that determines the 
+// subcommunication group that we belong in
+int rank_to_group_index(int rank,int ranks_per_comm,int extra_rank)
+{
+    int ret;
+
+    if (rank < (extra_rank*(ranks_per_comm+1))){
+        ret = rank / (ranks_per_comm+1);
+    }
+    else{
+        ret = (rank - extra_rank)/ranks_per_comm;
+    }
+
+    
+    return ret;
+}
+
+
+char *count_to_hostdir(Bit *bitmap,int stop_point,int *count,int *hostdir_found,
+        char *filename,char *target,int first){
+    
+    char hostdir_num[16];
+    
+    while((*hostdir_found)<stop_point){
+        if(bitmap[*count].bit==1) {
+            (*hostdir_found)++;
+        }
+        (*count)++;
+    }
+    if(!first) strcpy(filename,target);
+    if(first) strcat(filename,target);
+    strcat(filename,"/");
+    strcat(filename,HOSTDIR_PREFIX);
+    sprintf(hostdir_num,"%d",(*count)-1);
+    strcat(filename,hostdir_num);
+    return filename;
+}
+
+char* bitmap_to_dirname(Bit *bitmap,int group_index,
+        char *target,int mult,int np)
+{
+    char *path;
+    int count = 0;
+    int hostdir_found=0;
+    
+    if(mult==0){
+        path=malloc(sizeof(char)*4096);
+        path=count_to_hostdir(bitmap,group_index+1,&count,
+                &hostdir_found,path,target,0);        
+    }
+    else{
+        path=malloc(sizeof(char)*(mult*4096));
+        int dirs=0;
+        while(dirs<mult){
+            int stop_point;
+
+            stop_point=(group_index+1)+(dirs*np);
+            path = count_to_hostdir(bitmap,stop_point,&count,
+                        &hostdir_found,path,target,count);
+            strcat(path,"|");
+            dirs++;
+        }
+    }
+        
+    return path;
+}
+
--- /tmp/openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_read.c	1969-12-31 17:00:00.000000000 -0700
+++ ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_read.c	2011-01-31 16:39:35.000000000 -0700
@@ -0,0 +1,50 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/* 
+ *   $Id: ad_plfs_read.c,v 1.1 2010/11/29 19:59:01 adamm Exp $    
+ *
+ *   Copyright (C) 1997 University of Chicago. 
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "adio.h"
+#include "adio_extern.h"
+#include "ad_plfs.h"
+
+void ADIOI_PLFS_ReadContig(ADIO_File fd, void *buf, int count, 
+                     MPI_Datatype datatype, int file_ptr_type,
+		     ADIO_Offset offset, ADIO_Status *status, int *error_code)
+{
+    int err=-1, datatype_size, len, rank;
+    ADIO_Offset myoff;
+    static char myname[] = "ADIOI_PLFS_READCONTIG";
+
+    MPI_Type_size(datatype, &datatype_size);
+    len = datatype_size * count;
+    MPI_Comm_rank( fd->comm, &rank );
+
+    // for the romio/test/large_file we always get an offset of 0
+    // maybe we need to increment fd->fp_ind ourselves?
+    if (file_ptr_type == ADIO_EXPLICIT_OFFSET) {
+        myoff = offset;
+    } else {
+        myoff = fd->fp_ind;
+    }
+    plfs_debug( "%s: offset %ld len %ld rank %d\n", 
+            myname, (long)myoff, (long)len, rank );
+
+    err = plfs_read( fd->fs_ptr, buf, len, myoff );
+
+#ifdef HAVE_STATUS_SET_BYTES
+    if (err >= 0 ) MPIR_Status_set_bytes(status, datatype, err);
+#endif
+
+    if (err < 0 ) {
+	*error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+					   myname, __LINE__, MPI_ERR_IO,
+					   "**io",
+					   "**io %s", strerror(-err));
+    } else {
+        fd->fp_ind += err;
+        *error_code = MPI_SUCCESS;
+    }
+}
--- /tmp/openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_resize.c	1969-12-31 17:00:00.000000000 -0700
+++ ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_resize.c	2011-01-31 16:39:35.000000000 -0700
@@ -0,0 +1,34 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/* 
+ *   $Id: ad_plfs_resize.c,v 1.1 2010/11/29 19:59:01 adamm Exp $    
+ *
+ *   Copyright (C) 1997 University of Chicago. 
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "ad_plfs.h"
+
+void ADIOI_PLFS_Resize(ADIO_File fd, ADIO_Offset size, int *error_code)
+{
+    int err;
+    int rank;
+    static char myname[] = "ADIOI_PLFS_RESIZE";
+    plfs_debug( "%s: begin\n", myname );
+
+    /* because MPI_File_set_size is a collective operation, and PLFS1 clients
+     * do not cache metadata locally, one client can resize and broadcast the
+     * result to the others */
+    MPI_Comm_rank(fd->comm, &rank);
+    if (rank == fd->hints->ranklist[0]) {
+	err = plfs_trunc(fd->fs_ptr, fd->filename, size);
+    }
+    MPI_Bcast(&err, 1, MPI_INT, 0, fd->comm);
+
+    if (err < 0) {
+	*error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+					   myname, __LINE__, MPI_ERR_IO,
+					   "**io",
+					   "**io %s", strerror(-err));
+    }
+    else *error_code = MPI_SUCCESS;
+}
--- /tmp/openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_write.c	1969-12-31 17:00:00.000000000 -0700
+++ ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/ad_plfs_write.c	2011-01-31 16:39:35.000000000 -0700
@@ -0,0 +1,50 @@
+/* -*- Mode: C; c-basic-offset:4 ; -*- */
+/* 
+ *   $Id: ad_plfs_write.c,v 1.1 2010/11/29 19:59:01 adamm Exp $    
+ *
+ *   Copyright (C) 1997 University of Chicago. 
+ *   See COPYRIGHT notice in top-level directory.
+ */
+
+#include "ad_plfs.h"
+#include "adio_extern.h"
+
+
+void ADIOI_PLFS_WriteContig(ADIO_File fd, void *buf, int count, 
+			    MPI_Datatype datatype, int file_ptr_type,
+			    ADIO_Offset offset, ADIO_Status *status,
+			    int *error_code)
+{
+    int err=-1, datatype_size, len, rank;
+    ADIO_Offset myoff;
+    static char myname[] = "ADIOI_PLFS_WRITECONTIG";
+
+    MPI_Type_size(datatype, &datatype_size);
+    len = datatype_size * count;
+    MPI_Comm_rank( fd->comm, &rank );
+
+    // for the romio/test/large_file we always get an offset of 0
+    // maybe we need to increment fd->fp_ind ourselves?
+    if (file_ptr_type == ADIO_EXPLICIT_OFFSET) {
+        myoff = offset;
+    } else {
+        myoff = fd->fp_ind;
+    }
+    plfs_debug( "%s: offset %ld len %ld rank %d\n", 
+            myname, (long)myoff, (long)len, rank );
+    err = plfs_write( fd->fs_ptr, buf, len, myoff, rank );
+#ifdef HAVE_STATUS_SET_BYTES
+    if (err >= 0 ) MPIR_Status_set_bytes(status, datatype, err);
+#endif
+
+    if (err < 0 ) {
+        *error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,
+					   myname, __LINE__, MPI_ERR_IO,
+					   "**io",
+					   "**io %s", strerror(-err));
+    } else {
+        fd->fp_ind += err;
+        *error_code = MPI_SUCCESS;
+    }
+}
+
--- /tmp/openmpi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/Makefile.am	1969-12-31 17:00:00.000000000 -0700
+++ ompi-1.4.3/ompi/mca/io/romio/romio/adio/ad_plfs/Makefile.am	2011-01-31 16:39:35.000000000 -0700
@@ -0,0 +1,34 @@
+#
+# Copyright (c) 2004-2005 The Trustees of Indiana University and Indiana
+#                         University Research and Technology
+#                         Corporation.  All rights reserved.
+# Copyright (c) 2004-2005 The University of Tennessee and The University
+#                         of Tennessee Research Foundation.  All rights
+#                         reserved.
+# Copyright (c) 2004-2005 High Performance Computing Center Stuttgart, 
+#                         University of Stuttgart.  All rights reserved.
+# Copyright (c) 2004-2005 The Regents of the University of California.
+#                         All rights reserved.
+# Copyright (c) 2008      Cisco Systems, Inc.  All rights reserved.
+# $COPYRIGHT$
+# 
+# Additional copyrights may follow
+# 
+# $HEADER$
+#
+
+include $(top_srcdir)/Makefile.options
+
+noinst_LTLIBRARIES = libadio_plfs.la
+libadio_plfs_la_SOURCES = \
+	ad_plfs.c \
+	ad_plfs.h \
+	ad_plfs_close.c \
+	ad_plfs_delete.c \
+	ad_plfs_fcntl.c \
+	ad_plfs_flush.c \
+	ad_plfs_open.c \
+	ad_plfs_read.c \
+	ad_plfs_resize.c \
+	ad_plfs_write.c 
+
