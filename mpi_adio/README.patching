###############################################################################
        Patching an MPI implementation to make it PLFS aware
###############################################################################

In order to use PLFS as a ROMIO ADIO layer for MPI_IO, it is necessary to add
PLFS to the ROMIO of the MPI implementation. We are not so much patching MPI
as we are patching a version of the MPICH2 ROMIO layer. ROMIO is maintained in
the MPICH2 package; other MPI implementations such as Open MPI pull a version
of ROMIO from MPICH2 and place it in their own package.

The patching of an MPI implementation to make it PLFS aware requires that two
patches be applied to it. The first patch is one that patches the files that
already exist in the MPI implementation to let the MPI's build process know
about the PLFS layer. This patch is fairly static once developed for a
specific MPI's ROMIO layer. The second patch puts all of the needed PLFS code
in to the MPI implementation. As PLFS changes, this second patch may need to
change. In short, the first patch modifies existing MPI files, the second
patch copies over new files to the MPI implementation.


*** Patching and Building MPI ***

Only Open MPI has been extensively testes with PLFS. Other MPI implementations
should work and some instruction is provided below, but some modifications may
be required. Patches for MPICH2 are included, but are not extensively tested. 
They should also work for any MPICH2 derivative that doesn't modify the path
to the ROMIO layer (such as MVAPICH2).

1) make install of plfs
   a) If you are dealing with a checkout of the PLFS code rather than a
      tarball release, the second patch needs to be generated. Do the
      following to generate the patch (not needed when working from a release
      tarball):
      1) cd <plfs source directory>/mpi_adio
      2) ./scripts/make_ad_plfs_patch
      The default behavior of make_ad_plfs_patch is to create a patch for Open
      MPI. It is possible to create a patch for other MPI implementations.
      This is done by using make_ad_plfs_patch's --mpi command line parameter.
      Please see make_ad_plfs_patch's help for further information.
2) Get the MPI source (<mpi>-<version>)
3) cd <mpi>-<version> (need to be in the top-level source directory for the MPI
   implementation; <mpi> is an MPI implementation like openmpi, mpich2,
   mvapich2, etc.)
4) patch -p1 < /path/to/plfs/mpi_adio/patches/<mpi>/<mpi>-<version>-plfs-prep.patch
   This is the patch that modifies the existing files of an MPI implementation.
   Note that <mpi>-<version>-plfs-prep.patch may not exist for every version.
   There are prep.patch files that work for more than one version of an MPI
   implementation. These will have an 'x' in the version string to help signify
   which versions a particualr patch can be applied to. For example,
   ompi-1.4.x-plfs-pre.patch can be applied to any 1.4.x release such as 1.4.3
   or 1.4.5. Please see what patches are available for the desired MPI
   implementation by looking in patches/<mpi> where <mpi> is the desired MPI.
5) patch -p1 < /path/to/plfs/mpi_adio/patches/<mpi>/<mpi>-plfs.patch
   This patch puts in the PLFS-specific code in to the MPI implementation. It
   should work on any version of the needed MPI implementation.
6) Check the section labelled "Build Notes for specific versions of MPI" in
   this README to find out if anything special needs to be done for the
   desired MPI.
7) Since some of the templates for the MPI build have been modified by the
   above two patches (such as Makefile.am or Makefile.in), the build scripts
   need to be generated again. This is done by different methods depending on
   the MPI implementation. For convenience, here are methods for doing this
   with Open MPI and MPICH2 at the time of this writing:
   a) for Open MPI:
      ./autogen.sh
   b) for MPICH2 and derivatives:
      ./maint/updatefiles
   Please note that the autotools used for this step be at least the versions
   specified by documentation for that MPI.
8) Build MPI: ./configure; make; make install
   The MPI build needs to be able to find libplfs and use it. This is done by
   setting some environment variables and running the configure script.
   a) for Open MPI, set the following env variables

      LDFLAGS="-L/path/to/plfs/lib -Wl,-rpath=/path/to/plfs/lib -lplfs -lstdc++ -lpthread"
      CFLAGS="-I/path/to/plfs/include"
      CXXFLAGS="-I/path/to/plfs/include"
      CCASFLAGS="-I/path/to/plfs/include"

      Then, run configure with like this:

      ./configure --with-io-romio-flags=--with-file-system=ufs+nfs+plfs

      Other flags can be passed to configure such as prefix.
      It is also possible to put the above in a platform file and give that to
      Open MPI's configure process. Please see the Open MPI documentation for
      further information.

   b) for MPICH2 and its derivatives, set the LDFLAGS, CFLAGS, and CXXFLAGS
      env variables as with Open MPI and use the following configure command
      (with possibly more parameters passed to configure such as --prefix):

      ./configure --enable-romio --with-file-system=ufs+nfs+plfs

Here is an example of patching Open MPI 1.4.5 with a checkout of the PLFS code:
cd /path/to/plfs/mpi_adio # Don't need this if I was working with tarball release
./scripts/make_ad_plfs_patch # Don't need this if I was workign with tarball release
cd
tar xjf openmpi-1.4.5.tar.bz2
cd openmpi-1.4.5
patch -p1 < /path/to/plfs/mpi_adio/patches/openmpi/ompi-1.4.x-plfs-prep.patch
patch -p1 < /path/to/plfs/mpi_adio/patches/openmpi/ompi-plfs.patch
./autogen.sh
export LDFLAGS="-L/path/to/plfs/lib -Wl,-rpath=/path/to/plfs/lib -lplfs -lstdc++ -lpthread"
export CFLAGS="-I/path/to/plfs/include"
export CXXFLAGS="-I/path/to/plfs/include"
export CCASFLAGS="-I/path/to/plfs/include"
export CPPFLAGS="-DROMIO_OPENMPI_14x"
./configure --with_io_romio_flags=--with-file-system=ufs+nfs+plfs --prefix=/path/to/mpi/install
make
make install


###############################################################################
Build Notes for specific versions of MPI
###############################################################################
In order for some versions of MPI to build successfully using this version of
PLFS, some additional considerations may be necessary

*** Open MPI 1.4.x ***

The ROMIO in Open MPI 1.4.x is older. Some pieces of the PLFS ADIO code must
be left out. This is done by using the following directive: ROMIO_OPENMPI_14x.
Before running configure, set CPPFLAGS to include this directive:
export CPPFLAGS="-DROMIO_OPENMPI_14x"

*** Open MPI 1.6, 1.6.1, 1.6.2, 1.6.3 ***

There is a missing file in the release tarballs for Open MPI 1.6 through 1.6.3.
The file is ompi/mca/io/romio/romio/autogen.sh. This file is present in the
Open MPI repository branches for these releases, but it is not included when
creating the release tarball. Without this file, running the top-level
autogen.sh will break romio's configure script (m4 macros are left
un-substituted). The romio layer will not be built at all and it will not be
possible to do any MPI/IO.

To see if this behavior is happening, check the output from configure. Search
for romio. There will be a section for it as it tries to run romio's configure
script, but the following will be near the end of that section:

configure: /bin/sh './configure' *failed* for ompi/mca/io/romio/romio
configure: WARNING: ROMIO distribution did not configure successfully
checking if MCA component io:romio can compile... no

To fix this, it is necessary to replace romio's autogen.sh file. Either grab
the file from Open MPI's repository branch for that release, or use the
following (between the lines marked with ---) for the contents of the file:

---beginning of file---
:
autoreconf -ivf -I confdb
---end of file---

Save the file as ompi/mca/io/romio/romio/autogen.sh and make sure it is
executable. If it is not executable, it will be the same situation as if the
file were missing.

Once the missing autogen.sh file is in place, run Open MPI's top-level
autogen.sh as outlined in the general instructions for patching Open MPI and
continue with building as normal.
