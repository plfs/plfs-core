#ifndef __CONTAINER_ADIO_H_
#define __CONTAINER_ADIO_H_

/*
 * container_adio.h  PLFS MPI ADIO optimization functions
 *
 * container functions used by the plfs-core/mpi_adio code.
 * must provide C linkage for these fuctions, since the plfs
 * adio code is in C.
 */

#ifdef __cplusplus
extern "C" {
#endif
    
    /**
     * container_gethostdir_id: get the hostdir id number for a given
     * hostname.
     *
     * @param hostname name of host of interest
     * @return the id number the host maps to
     */
    extern size_t container_gethostdir_id(char *hostname); 

    /**
     * container_num_host_dirs: scans a directory that we believe is
     * a container and count how many hostdir.N hostdirs have been
     * allocated so far.  we use this to determine how to split up
     * the process of reading an index among our processors.
     *
     * @param hostdir_count the number of hostdirs is put here
     * @param target the directory to scan
     * @param vback C void* pointing to the C++ IOStore for the directory
     * @param bm bitmap generated by the "N"'s in present hostdir.N dirs
     * @return PLFS_SUCCESS, error (EISDIR if target is a dir, !container)
     */
    extern plfs_error_t container_num_host_dirs(int *hostdir_count,
                                                char *target, void *vback,
                                                char *bm);

    /**
     * container_hostdir_rddir: function called from MPI open when
     * #hostdirs>#procs.  this function is used under MPI (called only
     * by adplfs_read_and_merge).  the output of this function is a buffer
     * of serialized index records.
     * 
     * @param index_stream buffer to place result in
     * @param targets bpaths of hostdirs in canonical, sep'd with '|'
     * @param rank the MPI rank of caller
     * @param top_level bpath to canonical container dir
     * @param pmount void pointer to PlfsMount of logical file
     * @param pback void pointer to plfs_backend of canonical container
     * @param index_sz returns # output bytes in index_stream or -1
     * @return PLFS_SUCCESS or PLFS_E*
     */ 
    extern plfs_error_t container_hostdir_rddir(void **index_stream,
                                                char *targets, int rank,
                                                char *top_level, void *pmount,
                                                void *pback, int *index_sz);

    /**
     * container_hostdir_zero_rddir: called from MPI open when
     * #procs>#subdirs, so there are a set of procs assigned to one
     * subdir.  the comm has been split so there is a rank 0 for each
     * subdir.  each rank 0 calls this to resolve the metalink and get
     * the list of index files in this subdir.  note that the first
     * entry of the returned list is special and contains the 'path
     * holder' bpath of subdir (with all metalinks resolved -- see
     * indices_from_subdir).
     * 
     * @param entries ptr to resulting list of IndexFileInfo put here
     * @param path the bpath of hostdir in canonical container
     * @param rank top-level rank (not the split one)
     * @param pmount logical PLFS mount point where file being open resides
     * @param pback the the canonical backend
     * @param ret_size size of hostdir stream entries to return
     * @return PLFS_SUCCESS or PLFS_E*
     */ 
    extern plfs_error_t container_hostdir_zero_rddir(void **entries,
                                                     const char *path,
                                                     int rank, void *pmount,
                                                     void *pback,
                                                     int *ret_size);

    /**
     * container_parindex_read: called from MPI open's split and merge
     * code path to read a set of index files in a hostdir on a single
     * backend.
     * 
     * @param rank our rank in the split MPI communicator
     * @param ranks_per_comm number of ranks in the comm
     * @param index_files stream of IndexFileInfo recs from indices_from_subdir
     * @param index_stream resulting combined index stream goes here (output)
     * @param top_level bpath to canonical container
     * @param ret_index_size size of index to return
     * @return PLFS_SUCCESS or PLFS_E*
     */
    extern plfs_error_t container_parindex_read(int rank, int ranks_per_comm,
                                                void *index_files,
                                                void **index_stream,
                                                char *top_level,
                                                int *ret_index_size);

    /**
     * container_parindexread_merge: used by MPI open parallel index
     * read (both read_and_merge and split_and_merge cases) to take a
     * set of "procs" index streams in index_streams in memory and
     * merge them into one single index stream (the result is saved in
     * the "index_stream" pointer... this is all in memory, no threads
     * or I/O used.
     *
     * @param path loaded into Index object (XXX: is it used?)
     * @param index_streams array of input stream buffers to merge
     * @param index_sizes array of sizes of the buffers
     * @param procs the number of streams we are merging
     * @param index_stream buffer with resulting stream placed here
     * @return the number of bytes in the output stream
     */
    extern int container_parindexread_merge(const char *path,
                                            char *index_streams,
                                            int *index_sizes, int procs,
                                            void **index_stream);

    /**
     * container_index_stream: get a serialized index stream from an
     * open file.  if the file is open for read, we get it via the
     * index.  if the file is open for writing, we get it via the
     * WriteFile (which has an index for this purpose).  used by
     * flatten_then_close and adplfs_broadcast_index (the latter via
     * adplfs_open_helper).
     *
     * @param fd_in C void pointer to open file
     * @param buffer pointer to the output is placed here
     * @param ret_index_sz the size of the output buffer is place here
     * @return PLFS_SUCCESS on success, otherwise error code
     */
    extern plfs_error_t container_index_stream(Plfs_fd **pfd, char **buffer,
                                               int *ret_index_sz);

    /**
     * container_merge_indexes: used during flatten_then_close by the
     * rank 0 proc to merge all the indexes of all the ranks into one
     * global index that can be saved (further on in the
     * flatten_then_close code path).
     *
     * @param pfd the file currently in flatten_then_close processing
     * @param index_streams the index streams from all the procs in one buf
     * @param index_sizes the size of each proc's streams
     * @param procs the number of procs we have
     * @return always SUCCESS, the merged index is saved in pfd.
     */
    extern plfs_error_t container_merge_indexes(Plfs_fd **pfd,
                                                char *index_streams,
                                                int *index_sizes, int procs);

#ifdef __cplusplus
}
#endif

#endif
