${COPYRIGHT}
.TH plfsrc 5 "${PACKAGE_STRING}" 

.SH NAME
plfsrc \- the configuration file for PLFS (Parallel Log Structured File System)

.SH DESCRIPTION
This configuration file defines the mapping between logical and physical paths on the parallel filesystem.
It also has some configuration options that PLFS will use to try and optimize performance.  This file is never
written by PLFS, it is only read and it is the duty of the systems administrator or user to properly create
and maintain this file.  Please note that this configuration file is very important; changing it can potentially
disrupt the ability to read existing files.  If a plfsrc file must be changed, it is recommended to create a
new mount point and copy files over.  Additionally, please use the 'plfs_check_config' tool included in the

You can think of this file as essentially mount options.  However, we use a
configuration file because PLFS can be run without being mounted (e.g. using
MPI-IO). 
installation after every edit to the plfsrc file.  PLFS searches for this file in $ENV{'PLFSRC'}, $HOME/.plfsrc,
/etc/plfsrc in that order. 

.SH FILE FORMAT
The file consists of one keyword argument pair per line.  Configuration options must be separated by whitespace,
ie "keyword argument".  Any line with starting with a # is not interpreted.  Path names must be fully qualified.
Note that all keywords should be in lower-case.  The possible keywords and their meanings are as follows:

.B
include <file>
.RS
This includes <file> at the point of the include keyword; thereby allowing nested configuration files.
This is 
useful if you want to define global, consistent keyword definitions that can be used on different computers
needing to mount the same file systems but with some differences.  For example, interactive nodes and compute
nodes can share most of the values in an included shared configuration file but will independently define
different values for threadpool_size. 
.RE

.B
num_hostdirs
.RS
Official rule:
.br
Let N be the size in PE's of the largest job that will be run on the system.
.br
Let S be the sqrt of that.
.br
Let P be the lowest prime number greater than or equal to S.
.br
Set num_hostdirs to be P.

For example, say that the largest job that can be run on cluster C is 10K PEs.  num_hostdirs should be set to 101.

Explanation:
PLFS uses num_hostdirs to create balanced sub-directories.

Optional.  Default is 32.  Max is 1024.
.RE

.B
threadpool_size
.RS
This value is set to the number of threads to run on a machine; for machines used in large jobs, this value should
probably be around 2; for machines used in small jobs (like file transfer nodes), this value should probably be
around 16.

Optional.  Default is 8.
.RE

.B
index_buffer_mbs
.RS
This is the amount of memory (in megabytes) that PLFS can use to buffer indexing information while it is writing
files.  This buffer, if sufficiently large, enables subsequent optimizations which improve read open times.
This particular behavior is only available through the MPI-IO interface and can be controlled by an optional hint of
"plfs_flatten_close."  When setting this value, bear in mind that every open file handle can buffer this much
memory so that total memory being used can be much larger than this value.

Optional.  Default is 64.
.RE

.B
lazy_stat
.RS
The lazy_stat flag, when it is set as 0 plfs will do slow stat(getattr).  This is only for querying the size of
a file handle that is currently open for write.  Doing a non-lazy stat of this open file handle will be very
slow but will give an accurate answer.  Doing a lazy-stat of an open file handle will be very fast but might be 
inaccurate.  However, it will show monotonically increasing sizes.

Optional.  Default is 1.
.RE

.B
mount_point
.RS
The path which users use to access PLFS files.  Users should NEVER see the backends.  Note this option must
appear in the file before the backends keyword. The first backends keyword that follows this is the one that
defines the backends for this mount_point.

Required.
.RE

.B
backends
.RS
The location where you want the physical files that comprise a PLFS file to reside for each mount_point.
You can have a comma-separated list which will distribute PLFS workload across multiple backend directories.

Note this option must appear after the mount_point keyword and only applies to the mount_point command that it
follows.
Be careful: the backends directive is IMMUTABLE.  Once a mount point
is defined, the backends can NEVER be changed.  If multiple machines share a PLFS mount point, they must all
define the same exact set of backends in the same exact ordering.  When multiple machines share a mount 
point, it is recommended to use the include directive to allow you to define a
mount point and its backends in one file and then include it, making it impossible to have different configurations
on different machines.

Required.
.RE

.B
workload
.RS
This tells PLFS whether a specific mount_point is intended for either shared
file workloads (i.e. N-1) or file-per-proc workloads (i.e. N-N).  Possible
values are either "shared_file" or "file_per_proc".  "n-1" can also be used in
place of "shared_file" and "n-n" can also be used in
place of "file_per_proc.  shared_file is intended
for workloads where multiple writers write concurrently to a single file.  
Other workloads should use "file_per_proc" which should make a large metadata
improvement for most metadata workloads so long as multiple backends are defined for the mount point.
This is especially true if each backend is serviced by
a different metadata server so that the metadata workload can be balanced
across multiple servers.

Note this option must appear after the mount_point keyword and only applies to the mount_point command that it
follows.

Optional.  Default is shared_file.
.RE

.B
canonical_backends
.RS
This is an advanced directive and will not be used in typical installations.  See shadow_backends for a more
detailed description.

Note this option must appear after the mount_point keyword and only applies to the mount_point command that it
follows.

Optional.
.RE

.B
shadow_backends
.RS
This is an advanced directive and will not be used in typical installations.  This, along with canonical_backends, 
allows the user to specify that some set of backends are where canonical
containers should be placed and some different set of backends are where
shadows should be placed.  If these directives are used, then shadow subdirs
will ALWAYS be created in shadow backends.  This can be used in instances
where the canonical is desired to be on slow global storage devices and the data pieces are
desired to be on fast local storage devices.  In scenarios with lots of different fast local
mounts, different nodes will have different sets of shadow_backends defined
so that each node always writes to the fastest possible backend.  For reading,
this will require that all locations are visible everywhere.  For advanced development, the
PLFS team has used this to build "burst-buffer" systems.

Note this option must appear after the mount_point keyword and only applies to the mount_point command that it
follows.

Optional.
.RE

.B
syncer_ip
.RS
This is an advanced directive and will not be used in typical installations.  
This is used in conjunction with shadow_backends and canonical_backends.  It
is possible to use an RPC server to asynchronously copy data from shadow_backends
into canonical_backends by specifying this value and by calling plfs_protect.

Note this option must appear after the canonical_backends and/or shadow_backends keyword(s) and only applies
to the canonical_backends and/or shadow_backends command that it follows.

Optional.
.RE

.B
global_summary_dir
.RS
The path into which to drop summary information.  Useful to determine how
much PLFS is being used and a bit of information about how it is being
used.  This must be set to a globally writable and globally visible path.
When this is set, each close() will drop a file into this directory with
some statistical information in the filename.  This will happen on every
close() for every proc when using FUSE and just on rank 0's close() when
using ADIO.  Be careful if you use this because this directory can get
very full and can cause performance problems.

Optional.
.RE

.B
statfs
.RS
The path on which to resolve statfs calls received through FUSE.  Typically 
this is not specified but may be used to avoid hanging on mounts when 
backend volumes are not available.  This is because FUSE calls statfs when it
mounts and PLFS by default forwards that to one of the backends and this can
then hang. 

Note this option must appear after the mount_point keyword and only applies to the mount_point command that it
follows.

Optional.
.RE

.B
test_metalink
.RS
This is only for developers to do testing.  It won't be explained here.  If
you really want to know what it does, please read the code.  Otherwise, do
not use this directive.

Note the mount_point and backends may be specified multiple times in the same file.
Note that some commands are global and some are unique to a specific mount point.
This is to allow different backends declarations for each mount point for example.
Indeed it is an error to share a backend in multiple backends.  
.RE

.B
lazy_droppings
.RS
Plfs container tries to defer the creation of data droppings and index droppings
until the first write/truncate. Therefore if a file is opened and closed without
any writes in between, plfs doesn't need to create empty data/index droppings.

To disable the behavior, set lazy_droppings = 0.

Optional. The default is 1.
.RE

.B
mlog_defmask
.RS
The default logging level.  This is used to control how much logging
is enabled.   Possible values: EMERG, ALERT, CRIT, ERR, WARN, NOTE, INFO
and DEBUG.

Optional.  The default is WARN.
.RE

.B
mlog_setmasks
.RS
Resets the logging level for a subsystem (overwriting the default value).   
Currently defined subsystems are: plfs_misc, internal, container, index, 
writefile, fileops, utilities, store, FUSE, and MPI.  If the subsystem is 
omitted, then it sets the log mask for all subsystems.  For example, 
"INFO,index=DEBUG,container=DEBUG" sets the mask for all subsystems to
INFO, and then sets index and container to DEBUG.

Optional.
.RE

.B
mlog_stderrmask
.RS
Level at which log messages are printed to stderr.   Useful if set to
a higher priority than the normal mask.   This allows users to only
have a subset of higher priority log messages printed to stderr.

Optional.  The default value is CRIT.
.RE

.B
mlog_stderr
.RS
If set to 1, causes all logged messages to be printed to stderr, 
irrespective of the value of mlog_stderrmask (e.g. for debugging).

Optional.  The default value is 0.
.RE

.B
mlog_file
.RS
If set, then this contains the name of the PLFS log file.   If not
set (the default), then logs are not saved to a file.  Macros of %p, %h, and
%t can be put in the filename; they are expanded to PID, HOSTNAME, and UNIX
EPOCH respectively.

Optional.
.RE

.B
mlog_msgbuf_size
.RS
The size, in bytes of the in-memory log message buffer.  This is a
circular buffer containing the most recent logs.

Optional.  The default value is 4096.
.RE

.B
mlog_syslog
.RS
If set to 1, causes all logged messages to be sent the system's
syslog program with syslog(3).

Optional.  The default is 0.
.RE

.B
mlog_syslogfac
.RS
If mlog_syslog is 1, then this contains the facility level used to
open syslog with openlog(3).  It must be of the format LOCALn
where "n" is between 0 and 7.

Optional.  The default is to use the USER facility.
.RE

.B
mlog_ucon
.RS
A low-level debugging option that enables log messages to be sent
to a UDP socket (if set to 1).

Optional.  The default is 0.
.RE

Note that the mount_point and backends may be specified multiple times in the same file.

.br 
A tool for checking the plfsrc file for correct syntax is included in the plfs distribution.  See
.I plfs_check_config(1)
for more details. 

.SH EXAMPLE
A configuration file might appear as follows:
.P
num_hostdirs 32
.br
threadpool_size 16
.br
mount_point /tmp/plfs
.br
backends /tmp/.plfs_store

.SH FILES
.I /etc/.plfsrc
.RS
The system wide configuration file.
.RE
.I ~/.plfsrc
.RS
Per user configuration file.
.RE

The per user file options will override options in the system wide file. Options in files cannot be combined, only one file will be used.

.SH AUTHORS
${AUTHORS}

.SH SEE ALSO
${SEEALSO5}

